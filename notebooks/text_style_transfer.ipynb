{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Set up and Preprocessing**\n",
        "\n",
        "\n",
        "In Colab, you need to turn text into numbers that the LSTM can understand."
      ],
      "metadata": {
        "id": "gsIfm3jdPWCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Attention, Concatenate, Bidirectional, Dropout\n"
      ],
      "metadata": {
        "id": "SAJHdFXgOG8K"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. CONFIGURATION & PREPROCESSING ---\n",
        "MAX_LEN = 25\n",
        "EMBED_DIM = 256\n",
        "LATENT_DIM = 512\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = str(text).lower().strip()\n",
        "    # Reduce repeated characters (heyyyy -> heyy)\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "    # Space out punctuation\n",
        "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "wGUBJRWJi2_O"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "8Od2t_69N5HW"
      },
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "df = pd.read_csv('style_transfer_dataset (1).csv')\n",
        "df['normal_text'] = df['normal_text'].apply(normalize_text)\n",
        "# Add start/end tokens to target\n",
        "df['professional_text'] = df['professional_text'].apply(lambda x: f'sos {normalize_text(x)} eos')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynb2JxxIWhQM",
        "outputId": "c25b2fa2-8706-4a2c-e25e-d48d0d0b2eae"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "9lsep9GFOduf",
        "outputId": "fbe4185d-6080-4c3f-8091-30edf8104412"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             normal_text  \\\n",
              "913  could you possibly look into this ?   \n",
              "184   could you possibly send the file ?   \n",
              "796                would you handle this   \n",
              "887    i need you to review the document   \n",
              "380              would you send the file   \n",
              "\n",
              "                                     professional_text  \n",
              "913  sos your prompt attention to investigate this ...  \n",
              "184       sos please proceed to forward the file . eos  \n",
              "796  sos your prompt attention to manage this matte...  \n",
              "887    sos please proceed to review the document . eos  \n",
              "380  sos i would appreciate it if you could forward...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4db6b55f-b11e-4af8-b4e3-583284198c64\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>normal_text</th>\n",
              "      <th>professional_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>913</th>\n",
              "      <td>could you possibly look into this ?</td>\n",
              "      <td>sos your prompt attention to investigate this ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>could you possibly send the file ?</td>\n",
              "      <td>sos please proceed to forward the file . eos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>would you handle this</td>\n",
              "      <td>sos your prompt attention to manage this matte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>i need you to review the document</td>\n",
              "      <td>sos please proceed to review the document . eos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>would you send the file</td>\n",
              "      <td>sos i would appreciate it if you could forward...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4db6b55f-b11e-4af8-b4e3-583284198c64')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4db6b55f-b11e-4af8-b4e3-583284198c64 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4db6b55f-b11e-4af8-b4e3-583284198c64');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-69698cc6-1eee-4b86-947b-9db605464c4b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-69698cc6-1eee-4b86-947b-9db605464c4b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-69698cc6-1eee-4b86-947b-9db605464c4b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"normal_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"could you possibly send the file ?\",\n          \"would you send the file\",\n          \"would you handle this\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"professional_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"sos please proceed to forward the file . eos\",\n          \"sos i would appreciate it if you could forward the file . eos\",\n          \"sos your prompt attention to manage this matter would be valued . eos\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "def build_tokenizer(texts):\n",
        "    # We keep ! and ? because they matter for style\n",
        "    tokenizer = Tokenizer(filters='\"#$%&()*+,-/:;=@[\\\\]^_`{|}~\\t\\n', oov_token='<unk>')\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "input_tokenizer = build_tokenizer(df['normal_text'])\n",
        "target_tokenizer = build_tokenizer(df['professional_text'])\n",
        "\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "QaeJdm1GjI7H"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ğŸ—ï¸ Step 2: The Encoder-Decoder with Attention**\n",
        "\n",
        "\n",
        "We will build this using the TensorFlow Functional API. It consists of an Encoder (to understand the normal text) and a Decoder (to write the formal version)."
      ],
      "metadata": {
        "id": "gbKQTr7PPeTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Attention, Concatenate\n",
        "\n",
        "# Hyperparameters\n",
        "latent_dim = 256\n",
        "embed_dim = 128\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "\n"
      ],
      "metadata": {
        "id": "VPiGnxyeORgG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare sequences\n",
        "encoder_input_data = pad_sequences(input_tokenizer.texts_to_sequences(df['normal_text']), maxlen=MAX_LEN, padding='post')\n",
        "decoder_full_data = pad_sequences(target_tokenizer.texts_to_sequences(df['professional_text']), maxlen=MAX_LEN+1, padding='post')\n",
        "\n",
        "# Decoder Input: sos -> word1 -> word2\n",
        "# Decoder Target: word1 -> word2 -> eos\n",
        "decoder_input_data = decoder_full_data[:, :-1]\n",
        "decoder_target_data = decoder_full_data[:, 1:]"
      ],
      "metadata": {
        "id": "pkTfLhvwjX0P"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. MODEL ARCHITECTURE ---\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(MAX_LEN,))\n",
        "enc_emb = Embedding(input_vocab_size, EMBED_DIM)(encoder_inputs)\n",
        "encoder_lstm = Bidirectional(LSTM(LATENT_DIM, return_sequences=True, return_state=True, dropout=0.3))\n",
        "encoder_outputs, fh, fc, bh, bc = encoder_lstm(enc_emb)\n",
        "state_h = Concatenate()([fh, bh])\n",
        "state_c = Concatenate()([fc, bc])\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(MAX_LEN,))\n",
        "dec_emb_layer = Embedding(target_vocab_size, EMBED_DIM)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(LATENT_DIM * 2, return_sequences=True, return_state=True, dropout=0.3)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "# Attention\n",
        "attention_layer = Attention()\n",
        "context_vector = attention_layer([decoder_outputs, encoder_outputs])\n",
        "decoder_combined = Concatenate(axis=-1)([decoder_outputs, context_vector])\n",
        "\n",
        "# Output\n",
        "decoder_dense = Dense(target_vocab_size, activation='softmax')\n",
        "outputs = decoder_dense(decoder_combined)\n"
      ],
      "metadata": {
        "id": "TEPEY4xeOw02"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Model([encoder_inputs, decoder_inputs], outputs)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "              loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jFLj1_QYO4gW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**â³ Step 3: Training the Model**\n",
        "\n",
        "\n",
        "In Colab, you can now run the training loop. Since we have a small dataset, we will train for more epochs (like 50-100)."
      ],
      "metadata": {
        "id": "El42V0-NPl-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. TRAINING ---\n",
        "print(\"Starting training... aimed at generalizing, not just memorizing.\")\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=32,\n",
        "    epochs=100,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[callback]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPptoHwEj5AM",
        "outputId": "156fcc1f-a5cd-4d32-d271-8963fad9c6a8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training... aimed at generalizing, not just memorizing.\n",
            "Epoch 1/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 59ms/step - loss: 3.9045 - val_loss: 1.5214\n",
            "Epoch 2/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 1.5021 - val_loss: 1.3545\n",
            "Epoch 3/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 1.3457 - val_loss: 1.2587\n",
            "Epoch 4/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 1.2206 - val_loss: 1.1551\n",
            "Epoch 5/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 1.1065 - val_loss: 1.0436\n",
            "Epoch 6/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.9993 - val_loss: 0.9674\n",
            "Epoch 7/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.8953 - val_loss: 0.8756\n",
            "Epoch 8/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.8059 - val_loss: 0.7755\n",
            "Epoch 9/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.6911 - val_loss: 0.7124\n",
            "Epoch 10/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - loss: 0.6091 - val_loss: 0.6562\n",
            "Epoch 11/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.5748 - val_loss: 0.5935\n",
            "Epoch 12/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.4956 - val_loss: 0.5372\n",
            "Epoch 13/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.4542 - val_loss: 0.4981\n",
            "Epoch 14/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.3905 - val_loss: 0.4765\n",
            "Epoch 15/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.3485 - val_loss: 0.4491\n",
            "Epoch 16/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.3439 - val_loss: 0.4307\n",
            "Epoch 17/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.2939 - val_loss: 0.4325\n",
            "Epoch 18/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.2970 - val_loss: 0.4131\n",
            "Epoch 19/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.2862 - val_loss: 0.3937\n",
            "Epoch 20/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.2522 - val_loss: 0.3965\n",
            "Epoch 21/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.2395 - val_loss: 0.3843\n",
            "Epoch 22/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - loss: 0.2222 - val_loss: 0.3818\n",
            "Epoch 23/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.2235 - val_loss: 0.3825\n",
            "Epoch 24/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.2077 - val_loss: 0.3815\n",
            "Epoch 25/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.1969 - val_loss: 0.3828\n",
            "Epoch 26/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.1910 - val_loss: 0.3797\n",
            "Epoch 27/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.1877 - val_loss: 0.3816\n",
            "Epoch 28/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.1832 - val_loss: 0.3906\n",
            "Epoch 29/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.1824 - val_loss: 0.3827\n",
            "Epoch 30/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.1757 - val_loss: 0.3814\n",
            "Epoch 31/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.1687 - val_loss: 0.3892\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7802604b7410>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. ROBUST INFERENCE (TRANSLATION) ---\n",
        "def translate_to_formal(sentence):\n",
        "    clean_input = normalize_text(sentence)\n",
        "    input_seq = input_tokenizer.texts_to_sequences([clean_input])\n",
        "    input_padded = pad_sequences(input_seq, maxlen=MAX_LEN, padding='post')\n",
        "\n",
        "    # Initialize decoder input with 'sos'\n",
        "    target_seq = np.zeros((1, MAX_LEN))\n",
        "    target_seq[0, 0] = target_tokenizer.word_index.get('sos', 1)\n",
        "\n",
        "    translated_words = []\n",
        "\n",
        "    for i in range(MAX_LEN - 1):\n",
        "        output = model.predict([input_padded, target_seq], verbose=0)\n",
        "\n",
        "        # Get the word for the current index i\n",
        "        idx = np.argmax(output[0, i, :])\n",
        "        word = target_tokenizer.index_word.get(idx, '')\n",
        "\n",
        "        if word == 'eos' or word == '' or idx == 0:\n",
        "            break\n",
        "\n",
        "        translated_words.append(word)\n",
        "        target_seq[0, i+1] = idx # Feed the predicted word back for next step\n",
        "\n",
        "    return \" \".join(translated_words)\n"
      ],
      "metadata": {
        "id": "38TZ5fU3kBNs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dD2mXIgldky5"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e4c77de"
      },
      "source": [
        "# Task\n",
        "Adjust hyperparameters, rebuild, and retrain the style transfer model, then evaluate its performance on unseen data to determine if reducing model capacity mitigates overfitting and leads to more accurate and formal translations. Provide an assessment of the model's performance and suggest any further steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c75d5e7"
      },
      "source": [
        "## Adjust Hyperparameters\n",
        "\n",
        "### Subtask:\n",
        "Reduce the model's capacity by adjusting the `LATENT_DIM` and `EMBED_DIM` hyperparameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68c5fa3b"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires adjusting the `EMBED_DIM` and `LATENT_DIM` hyperparameters. I will update these values in a new code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0874fdcb",
        "outputId": "2a1e4ef8-c5a7-4d22-d525-d0711c6cdfa1"
      },
      "source": [
        "EMBED_DIM = 128\n",
        "LATENT_DIM = 256\n",
        "\n",
        "print(f\"Updated EMBED_DIM: {EMBED_DIM}\")\n",
        "print(f\"Updated LATENT_DIM: {LATENT_DIM}\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated EMBED_DIM: 128\n",
            "Updated LATENT_DIM: 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f54e858b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step updated the `EMBED_DIM` and `LATENT_DIM` hyperparameters. Now, the model architecture needs to be re-defined and re-compiled using these new values to reflect the reduced capacity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce78648c"
      },
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Attention, Concatenate, Bidirectional, Dropout\n",
        "\n",
        "# --- 2. MODEL ARCHITECTURE ---\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(MAX_LEN,))\n",
        "enc_emb = Embedding(input_vocab_size, EMBED_DIM)(encoder_inputs)\n",
        "encoder_lstm = Bidirectional(LSTM(LATENT_DIM, return_sequences=True, return_state=True, dropout=0.3))\n",
        "encoder_outputs, fh, fc, bh, bc = encoder_lstm(enc_emb)\n",
        "state_h = Concatenate()([fh, bh])\n",
        "state_c = Concatenate()([fc, bc])\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(MAX_LEN,))\n",
        "dec_emb_layer = Embedding(target_vocab_size, EMBED_DIM)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(LATENT_DIM * 2, return_sequences=True, return_state=True, dropout=0.3)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "# Attention\n",
        "attention_layer = Attention()\n",
        "context_vector = attention_layer([decoder_outputs, encoder_outputs])\n",
        "decoder_combined = Concatenate(axis=-1)([decoder_outputs, context_vector])\n",
        "\n",
        "# Output\n",
        "decoder_dense = Dense(target_vocab_size, activation='softmax')\n",
        "outputs = decoder_dense(decoder_combined)\n",
        "\n",
        "model = tf.keras.Model([encoder_inputs, decoder_inputs], outputs)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "              loss='sparse_categorical_crossentropy')\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb833636"
      },
      "source": [
        "## Rebuild and Retrain Model\n",
        "\n",
        "### Subtask:\n",
        "Retrain the model with the adjusted hyperparameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "984ac093"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous steps updated the hyperparameters and rebuilt the model. Now, I need to retrain the model using the adjusted hyperparameters, following the instructions for batch size, epochs, validation split, and callbacks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cff52a8",
        "outputId": "6f24e851-e2ea-4968-fb7d-71ce3b6f01dd"
      },
      "source": [
        "print(\"Starting training with adjusted hyperparameters...\")\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=32,\n",
        "    epochs=60,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[callback]\n",
        ")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with adjusted hyperparameters...\n",
            "Epoch 1/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 4.3937 - val_loss: 1.5796\n",
            "Epoch 2/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.5274 - val_loss: 1.4070\n",
            "Epoch 3/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.3723 - val_loss: 1.3099\n",
            "Epoch 4/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.2753 - val_loss: 1.2148\n",
            "Epoch 5/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.1838 - val_loss: 1.1392\n",
            "Epoch 6/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.0946 - val_loss: 1.0603\n",
            "Epoch 7/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.0180 - val_loss: 0.9820\n",
            "Epoch 8/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.9325 - val_loss: 0.9117\n",
            "Epoch 9/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.8617 - val_loss: 0.8499\n",
            "Epoch 10/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.7824 - val_loss: 0.7933\n",
            "Epoch 11/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.7009 - val_loss: 0.7538\n",
            "Epoch 12/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.6590 - val_loss: 0.6967\n",
            "Epoch 13/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.6080 - val_loss: 0.6481\n",
            "Epoch 14/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.5515 - val_loss: 0.6122\n",
            "Epoch 15/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5113 - val_loss: 0.5926\n",
            "Epoch 16/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.4935 - val_loss: 0.5603\n",
            "Epoch 17/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.4865 - val_loss: 0.5330\n",
            "Epoch 18/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.4355 - val_loss: 0.5237\n",
            "Epoch 19/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.4298 - val_loss: 0.5030\n",
            "Epoch 20/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.4059 - val_loss: 0.4895\n",
            "Epoch 21/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3790 - val_loss: 0.4761\n",
            "Epoch 22/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3605 - val_loss: 0.4688\n",
            "Epoch 23/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3519 - val_loss: 0.4579\n",
            "Epoch 24/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.3418 - val_loss: 0.4478\n",
            "Epoch 25/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3271 - val_loss: 0.4429\n",
            "Epoch 26/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3104 - val_loss: 0.4362\n",
            "Epoch 27/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3052 - val_loss: 0.4372\n",
            "Epoch 28/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2957 - val_loss: 0.4302\n",
            "Epoch 29/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2825 - val_loss: 0.4205\n",
            "Epoch 30/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2656 - val_loss: 0.4211\n",
            "Epoch 31/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2607 - val_loss: 0.4188\n",
            "Epoch 32/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.2493 - val_loss: 0.4114\n",
            "Epoch 33/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2479 - val_loss: 0.4086\n",
            "Epoch 34/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2296 - val_loss: 0.4096\n",
            "Epoch 35/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2147 - val_loss: 0.4017\n",
            "Epoch 36/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2228 - val_loss: 0.4110\n",
            "Epoch 37/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.2197 - val_loss: 0.3996\n",
            "Epoch 38/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2163 - val_loss: 0.3998\n",
            "Epoch 39/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2021 - val_loss: 0.4005\n",
            "Epoch 40/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.2003 - val_loss: 0.3953\n",
            "Epoch 41/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.2087 - val_loss: 0.3993\n",
            "Epoch 42/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.1863 - val_loss: 0.3963\n",
            "Epoch 43/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.1807 - val_loss: 0.3991\n",
            "Epoch 44/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.1812 - val_loss: 0.3993\n",
            "Epoch 45/60\n",
            "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1831 - val_loss: 0.4009\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x780200199970>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59e1b21f"
      },
      "source": [
        "**Reasoning**:\n",
        "The model has been retrained with adjusted hyperparameters. Now, I need to evaluate its performance on unseen data and provide an assessment of its performance, comparing it to the previous training results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c274fc7f",
        "outputId": "4e86665b-836b-40ad-8076-2e8075f82e9b"
      },
      "source": [
        "print(\"\\n\" + \"=\"*30)\n",
        "test_inputs = [\n",
        "    \"hey how are you??\",\n",
        "    \"send me the file ASAP\",\n",
        "    \"i'm so tired of this project\",\n",
        "    \"let's talk tomorrow\"\n",
        "]\n",
        "\n",
        "for t in test_inputs:\n",
        "    result = translate_to_formal(t)\n",
        "    print(f\"Casual: {t}\")\n",
        "    print(f\"Formal: {result}\")\n",
        "    print(\"-\" * 10)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Casual: hey how are you??\n",
            "Formal: i will respond the delayed of this\n",
            "----------\n",
            "Casual: send me the file ASAP\n",
            "Formal: i would appreciate it if you could review the document .\n",
            "----------\n",
            "Casual: i'm so tired of this project\n",
            "Formal: i am concerns regarding this approach\n",
            "----------\n",
            "Casual: let's talk tomorrow\n",
            "Formal: this is executed well\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "GydV7EGMqBy1"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_ETz9M8Zxp59"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}